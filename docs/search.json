[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hands-On: Ocean Fronts Workshop 2024",
    "section": "",
    "text": "About this book\nOur interest in ocean fronts as climate change biologists began because these are ephemeral ecosystems that are best managed dynamically, offering the potential for management responses that can automatically adapt to climate change.\nOcean fronts are critical sources of productivity and food for a wide range of species across all oceans. This makes their conservation important not just as a response to climate change but as a key feature for many species, including dozens of conservation-concern megafauna.\nBuilding on pioneering work in understanding the biology of ocean front systems, we aimed to explore the conservation implications and potential management strategies for these fronts in the context of climate change. While dynamic protection measures are certainly an intriguing aspect of this work, ocean fronts are deserving of conservation attention due to their importance to megafauna, including key fishery species like tuna. Consequently, a better understanding of ocean fronts can enhance both the conservation of species of concern and improve fisheries outcomes.\nCertain areas of the world stand out as ocean front “hotspots”. The Mozambique Channel, the North Atlantic, the southern coast of Argentina, and the vast band of fronts surrounding the Southern Ocean are all regions where these crucial features are most prominent. The users of these ocean fronts include a “Who’s Who” of charismatic megafauna and economically important species, from whales to seabirds, and from tuna to sea turtles.\nThis eBook has evolved from a series of workshops that matched ocean fronts with megafauna satellite tagging and observational data. It is designed to be useful to individual researchers beyond the workshop setting, and for those with advanced data skills, it serves as a standalone resource that can help assess megafauna use of both thermal and velocity fronts.\nWe are only scratching the surface—fronts have complex three-dimensional physical and biogeochemical structures that play critical roles in spatial use by megafauna. Our understanding of the dynamics of these subsurface structures and their utilization is still limited. However, mapping megafauna use at the surface, using satellite-derived temperature and velocity indices, is a good starting point.\nWe hope this eBook serves as an introduction to this fascinating world, inspiring researchers to unlock the full secrets of ocean fronts at depth, and spurring management innovations that can help conserve these unique, ephemeral systems and their critically important residents."
  },
  {
    "objectID": "01_preparation.html#open-science-nceas",
    "href": "01_preparation.html#open-science-nceas",
    "title": "1  Welcome!",
    "section": "1.1 Open Science: NCEAS",
    "text": "1.1 Open Science: NCEAS\nWith this material, we are following the NCEAS data approach to make research more transparent and reproducible, which enhances the credibility, utility, and accuracy of the science used to solve global challenges.\n\n\n\n\n\n\nNote\n\n\n\nOpen science is the philosophy and practice of making data and methods accessible, replicable, and free to use, typically through computer programming tools and techniques. It helps researchers compile and analyze data more efficiently and identify solutions more quickly."
  },
  {
    "objectID": "01_preparation.html#acknowledgments",
    "href": "01_preparation.html#acknowledgments",
    "title": "1  Welcome!",
    "section": "1.2 Acknowledgments",
    "text": "1.2 Acknowledgments\nThe authors acknowledge funding from the Ocean Front Change project, supported by the Belmont Forum and implemented through the National Science Foundation, NSF Award 2029710.\nAll code in this book was authored by Isaac Brito-Morales (ibrito@conservation.org). Please do not distribute this ebook without permission.\n\n\n\n\n\n\nDisclaimer 2\n\n\n\nNo Guarantees on Code Accuracy!\nCaveat Emptor! (Latin for “Let the buyer beware”)"
  },
  {
    "objectID": "01_preparation.html#about-this-book",
    "href": "01_preparation.html#about-this-book",
    "title": "1  Welcome!",
    "section": "1.3 About This Book",
    "text": "1.3 About This Book\nThis is a Quarto book. To learn more about Quarto books, visit Quarto Documentation."
  },
  {
    "objectID": "02_set_up.html#r",
    "href": "02_set_up.html#r",
    "title": "2  Setting up your computer",
    "section": "2.1 R",
    "text": "2.1 R\nThe R statistical computing environment can be downloaded from the Comprehensive R Archive Network (CRAN). Specifically, you can download the latest version of R (version 4.2.3) from here: https://cloud.r-project.org. Please note that you will need to download the correct file for your operating system (i.e. Linux, Mac OSX, Windows)."
  },
  {
    "objectID": "02_set_up.html#rstudio",
    "href": "02_set_up.html#rstudio",
    "title": "2  Setting up your computer",
    "section": "2.2 RStudio",
    "text": "2.2 RStudio\nRStudio is an integrated development environment (IDE). In other words, it is a program that is designed to make your R programming experience more enjoyable. During this workshop, you will interact with R through RStudio—meaning that you will open RStudio to code in R. You can download the latest version of RStudio here: http://www.rstudio.com/download. When you start RStudio, you will see two main parts of the interface:\n\n\n\n\n\nYou can type R code into the Console and press the enter key to run code."
  },
  {
    "objectID": "02_set_up.html#r-packages",
    "href": "02_set_up.html#r-packages",
    "title": "2  Setting up your computer",
    "section": "2.3 R packages",
    "text": "2.3 R packages\nAn R package is a collection of R code and documentation that can be installed to enhance the standard R environment with additional functionality. Currently, there are over fifteen thousand R packages available on CRAN. Each of these R packages are developed to perform a specific task, such as reading Excel spreadsheets, downloading satellite imagery data, downloading and cleaning protected area data, or fitting environmental niche models.\n\n\n\n\n\n\nNavigating R\n\n\n\nR has such a diverse ecosystem of R packages, that the question is almost always not “can I use R to …?” but “what R package can I use to …?”.\n\n\nDuring this workshop, we will use several R packages. To install these R packages, please enter the code below in the Console part of the RStudio interface and press enter. Note that you will require an Internet connection and the installation process may take some time to complete.\n\ninstall.packages(c(\"sf\", \"terra\", \"dplyr\", \"sp\", \"rgeos\", \"rgdal\", \"raster\",\n                   \"units\", \"tidyr\", \"stringr\", \"readr\", \"transformr\", \"data.table\",\n                   \"ggplot2\", \"RColorBrewer\", \"rnaturalearth\", \"rnaturalearthdata\",\n                   \"ggtext\", \"lwgeom\", \"patchwork\", \"gganimate\", \"animation\"))\n\n# Optional packages\n  # library(ncdf4)\n  # library(ncdf4.helpers)\n  # library(PCICt)\n  # library(magrittr)\n  # library(exactextractr)\n  # library(nngeo)"
  },
  {
    "objectID": "02_set_up.html#create-an-r-project",
    "href": "02_set_up.html#create-an-r-project",
    "title": "2  Setting up your computer",
    "section": "2.4 Create an R Project",
    "text": "2.4 Create an R Project\nIn this workshop, we’ll use an R project to efficiently organize your work and enhance collaboration. An R project links directly to a directory on your computer, streamlining file management and teamwork.\nUsing an R project is a best practice for reproducible research because it keeps all your work within a single directory. Think about your current workflow: where do you import, clean, analyze data, create graphs, and produce reports? Are you jumping between multiple tools like Excel, JMP, and Google Docs? With an R project, everything can be done and updated in one place — RStudio — streamlining your entire process.\n\n\n\n\n\n\nExample: R project set up\n\n\n\n\nIn the File menu, select New Project\nClick New Directory\nClick New Project\nUnder Directory name type: ofc_workshop_{USERNAME}\nLeave Create Project as subdirectory of: set to \\~\nClick Create Project\n\n\n\n\n2.4.1 Paths & Working Directories\nTwo types of paths: absolute paths and relative paths\n\nAbsolute path: starts with the root of your file system and ““locates files from there. The absolute path to this ebook in my computer is: /Users/ibrito/Desktop/NACECC_workshop/\nRelative paths: starts from some location in your file system that is below the root. R refer to the location where the relative path starts as our working directory.\n\n\n\n\n\n\n\nImportant\n\n\n\nRStudio projects automatically set the working directory to the directory of the project.\n\n\n\n\n\n\n\n\nDitch the setwd()\n\n\n\n\n\nonce you start working in projects you should basically never need to run the setwd() command. If you are in the habit of doing this, stop and take a look at where and why you do it."
  },
  {
    "objectID": "03_git_github.html#git",
    "href": "03_git_github.html#git",
    "title": "3  What exactly are Git and GitHub?",
    "section": "3.1 Git",
    "text": "3.1 Git\n\nan open-source distributed version control software\ndesigned to manage the versioning and tracking of source code files and project history\noperates locally on your computer, allowing you to create repositories, and track changes\nprovides features such as committing changes, branching and merging code, reverting to previous versions, and managing project history\nworks directly with the files on your computer and does not require a network connection to perform most operations\nprimarily used through the command-line interface (CLI, e.g. Terminal), but also has various GUI tools available (e.g. RStudio IDE)"
  },
  {
    "objectID": "03_git_github.html#github",
    "href": "03_git_github.html#github",
    "title": "3  What exactly are Git and GitHub?",
    "section": "3.2 GitHub",
    "text": "3.2 GitHub\n\nonline platform and service built around Git\nprovides a centralized hosting platform for Git repositories\nallows us to store, manage, and collaborate on their Git repositories in the cloud\noffers additional features on top of Git, such as a web-based interface, issue tracking, project management tools, pull requests, code review, and collaboration features\nenables easy sharing of code with others, facilitating collaboration and contribution to open source projects\nprovides a social aspect, allowing users to follow projects, star repositories, and discover new code"
  },
  {
    "objectID": "03_git_github.html#creating-github-repository",
    "href": "03_git_github.html#creating-github-repository",
    "title": "3  What exactly are Git and GitHub?",
    "section": "3.3 Creating GitHub repository",
    "text": "3.3 Creating GitHub repository\nAs always, there are different ways to skin a cat here! We’ll explore some of them, and you can decide later which best fits your style.\n\n3.3.1 Using the GitHub Website\n\n\n\n\n\n\nSetup\n\n\n\n\nLogin to GitHub\nClick the New repository button\nName it {FIRSTNAME}_delete\nAdd a short description\nCheck the box to add a README.md file\nAdd a .gitignore file using the R template\nSet the LICENSE to Apache 2.0\n\n\n\n\n\n3.3.2 Using the GitHub Desktop app\nThe GitHub Desktop app is a user-friendly application that allows you to interact with GitHub repositories from your local computer without using the command line. It provides a graphical interface to perform common GitHub tasks, such as:\n\nCloning repositories: Easily download repositories from GitHub to your local machine.\nCommitting changes: Make changes to files and save them with a message describing what you’ve done.\nPushing and pulling changes: Sync your local changes with the remote repository on GitHub and fetch the latest updates from collaborators.\nBranch management: Create, switch, merge, and delete branches to manage different versions or features of your project.\nResolving merge conflicts: Visually resolve conflicts that occur when merging changes from different branches.\n\nThe app simplifies the workflow for those who prefer a graphical interface over the command line, making GitHub more accessible to users of all experience levels. It is available for both Windows and macOS.\n\n\n\n\n\n\nSetup\n\n\n\n\nGo to GitHub Desktop\nClick “Download” and select your operating system.\nInstall the file on your local computer."
  },
  {
    "objectID": "04_cloning_repo.html#clone-the-workshop-repository",
    "href": "04_cloning_repo.html#clone-the-workshop-repository",
    "title": "4  Getting Started: workshop repository",
    "section": "4.1 clone the workshop repository",
    "text": "4.1 clone the workshop repository\nThere are several ways to clone a repository. Here, I’ll show you two easy methods: using the RStudio console and the GitHub Desktop app. You can also clone a repository using the terminal and command line, but we won’t cover that method in this workshop as it’s outside our scope—although, I’ve included a little tip below on how to do it! :-)\n\n\n\n\n\n\nclone a GitHub repository using the terminal:\n\n\n\n\n\n\nOpen the Terminal\n\n\nWindows: Press Win + R, type cmd, and press Enter. Alternatively, use Git Bash if you have it installed.\nmacOS: Press Cmd + Space, type Terminal, and press Enter.\nLinux: Open the terminal from the applications menu or press Ctrl + Alt + T.\n\n\nNavigate to the Directory Where You Want to Clone the Repository\n\n\nUse the cd command to navigate to the directory where you want to clone the repository. For example: cd path/to/your/directory\n\n\nCopy the Repository URL\n\n\nGo to the GitHub repository you want to clone: https://github.com/IsaakBM/NACECC_workshop_repo\nClick the green “Code” button and copy the URL (either HTTPS or SSH).\n\n\nClone the Repository\n\n\nIn the terminal, use the git clone command followed by the repository URL. For example: git clone https://github.com/IsaakBM/NACECC_workshop_repo.git\n\n\nNavigate into the Cloned Repository\n\n\nOnce the repository is cloned, navigate into its directory: cd NACECC_workshop_repo.git\n\n\nVerify the Clone\n\n\nTo verify that the repository has been cloned successfully, list the contents of the directory: ls -l\n\n\n\n\n\n4.1.1 Using the RStudio tool\nTo clone a repository using the RStudio, first go to the GitHub repository of the workshop https://github.com/IsaakBM/NACECC_workshop_repo and click the green “Code” button and copy the URL.\n\n\n\n\n\nThe next steps will look similar to those you followed when first creating an R Project, with a slight difference. Follow the instructions in the Setup box below to clone your remote repository to your local computer in RStudio:\n\n\n\n\n\n\nSetup using RStudio\n\n\n\n\nClick File &gt; New Project\nSelect Version Control and paste the remote repository URL (which should be copied to your clipboard) in the Repository ULR field\nPress Tab, which will auto-fill the Project directory name field with the same name as that of your remote repo – while you can name the local copy of the repository anything, it’s typical (and highly recommended) to use the same name as the GitHub repository to maintain the correspondence\n\n\n\n\n\n\n\n\n\n\n4.1.2 Using the Github Desktop app\nFollow the instructions in the Setup box below to clone your remote repository to your local computer using the GitHub Desktop app:\n\n\n\n\n\n\nSetup using Github Desktop app\n\n\n\n\nOpen GitHub Desktop.\nClick on File &gt; Clone Repository.\nIn the Clone a Repository dialog, go to the URL tab.\nPaste the remote repository URL (which should be copied to your clipboard) into the URL field.\nChoose the Local Path where you want to save the cloned repository. You can click Choose… to select a specific folder.\nClick Clone to create a local copy of the repository on your computer.\n\n\n\n\n\n\n\n\n\n\n4.1.3 Files in the clone repository\nDepending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present – you should see a Git tab, as well as the Files tab, where you can view all of the files copied from the remote repo to this local repo.\n\n\n\n\n\nYou’ll note that there is one new NACECC_workshop_repo.Rproj, and three files that we created earlier on GitHub (.gitignore, LICENSE, and README.md).\n\n\n\n\n\n\nabout the new Git files created\n\n\n\n\nA .gitignore: This file lists the files and directories that Git should ignore. It prevents unnecessary or sensitive files (like temporary files or local configuration files) from being tracked in your repository.\nLICENSE: This file specifies the legal terms under which others can use, modify, and distribute your code. It’s important to include a license to clarify the permissions for using your project.\nREADME.md: This file provides an overview of your project, including what it does, how to install or use it, and any other relevant information. It’s usually the first file people read when they visit your repository."
  },
  {
    "objectID": "04_cloning_repo.html#git-large-file-system",
    "href": "04_cloning_repo.html#git-large-file-system",
    "title": "4  Getting Started: workshop repository",
    "section": "4.2 Git large file system",
    "text": "4.2 Git large file system\nGit and GitHub are not intended to be platforms for storing data, but rather for developing software, creating reproducible analyses, and facilitating collaboration. However, as scientists (and I admit, I do this myself), we often store data for our projects, papers, etc., in repositories.\nKeep in mind that GitHub has a file size limit of 100 MB per file when uploading via the web interface and 2 GB per file when using Git. It’s generally recommended to keep your repository size under 1 GB to ensure good performance.\nTo handle larger files more effectively, we can use Git Large File Storage (LFS), which allows us to store pointers to large files in the repository while keeping the actual data stored elsewhere, providing more flexibility for managing larger datasets.\n\n\n\n\n\n\nSetup Git LFS files\n\n\n\n\nOpen Terminal\nNavigate to your repository by changing the working directory to where your repository is located:\n\n\ncd /Users/ibrito/Desktop/NACECC_workshop\n\n\nTrack files with Git LFS: To associate a specific file type with Git LFS, use the git lfs track command followed by the file extension. For example, to track netCDF file (i.e., .nc file):\n\n\ngit lfs track \"*.nc\"\n\n\nRepeat for each file type you want to track. This command will update your repository’s .gitattributes file to handle large files efficiently with Git LFS.\n\n\n\nIn RStudio, your project should now include a .gitattributes file, which will appear in your file list.\n\n\n\n\n\n\n\n\n\n\n\ninstalling Git LFS on macOS and Windows:\n\n\n\n\n\n\nmacOS\n\n\nOpen the Terminal.\nInstall Git LFS using Homebrew or MacPorts:\n\n\nbrew install git-lfs\nport install git-lfs\n\n\nInitialize Git LFS:\n\n\ngit lfs install\n\n\nWindows\n\n\nGo to the Git LFS website\nDownload the Git LFS installer for Windows\nRun the installer and follow the on-screen instructions\nOpen the terminal (Command Prompt or Git Bash) and run:\n\n\ngit lfs install"
  },
  {
    "objectID": "05_climate_data.html#cdo-installation-process",
    "href": "05_climate_data.html#cdo-installation-process",
    "title": "5  Getting Started with Climate Data",
    "section": "5.1 CDO Installation Process",
    "text": "5.1 CDO Installation Process\n\n5.1.1 MacOS\nFollow the instruction and downloaded MacPorts. MacPorts is an open-source community initiative to design an easy-to-use system for compiling, installing, and upgrading the command-line on the Mac operating system.\nMacPorts website MacPorts download\n\n\n\n\n\n\nSetup CDO in MacOS (Updated Instructions)\n\n\n\n1. Install CDO:\n\nAfter MacPorts installation, open the Terminal and type\n\n\nsudo port install cdo\n\n\nEnter your password\n\n2. Verify the Installation:\n\nTo verify that CDO is installed, type:\n\n\ncdo --version\n\nThis command will display the installed version of CDO\n\n\n\n\n\n\n\n\nHomebrew Alternative\n\n\n\nYou can use Homebrew, another popular package manager for macOS. To install CDO with Homebrew:\n\nbrew install cdo\n\n\n\n\n\n5.1.2 Windows\nIn the current Windows version(s) Microsoft includes an Ubuntu 16.04 LTS embedded Linux. This environment offers a clean integration with the windows file systems and and the opportunity to install CDO via the native package manager of Ubuntu.\n\n\n\n\n\n\nSetup CDO in Windows (Updated Instructions)\n\n\n\n1. Enable Windows Subsystem for Linux (WSL):\n\nOpen PowerShell as Administrator and run:\n\n\nwsl --install\n\n\nThis command installs the Windows Subsystem for Linux along with the latest Ubuntu distribution available from the Microsoft Store. If you already have WSL installed but want to upgrade or change your Linux distribution, you can manually select it from the Microsoft Store.\n\n2. Install Ubuntu Distribution:\n\nGo to the Microsoft Store and search for Ubuntu\nInstall the latest version of Ubuntu (e.g., Ubuntu 20.04 LTS or Ubuntu 22.04 LTS)\n\n3. Open the Ubuntu Terminal:\n\nOnce installed, open the Ubuntu terminal from the Start menu\n\n4. Update and Install CDO:\n\nIn the Ubuntu terminal, update your package list:\n\n\nsudo apt update\n\n\nInstall CDO using the package manager:\n\n\nsudo apt install cdo\n\n\nEnter your password when prompted\n\n5. Verify the Installation:\n\nTo verify that CDO is installed, type:\n\n\ncdo --version\n\nThis command will display the installed version of CDO\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nWindows Subsystem for Linux 2 provides a full Linux kernel and better performance. If you’re using an older version, you might want to upgrade to WSL 2 by following instructions on the WSL 2 documentation page\nThe Ubuntu terminal has seamless access to your Windows file system, so you can use CDO on files stored on your Windows drives directly"
  },
  {
    "objectID": "05_climate_data.html#ncview-a-netcdf-visual-browser",
    "href": "05_climate_data.html#ncview-a-netcdf-visual-browser",
    "title": "5  Getting Started with Climate Data",
    "section": "5.2 ncview: a netCDF visual browser",
    "text": "5.2 ncview: a netCDF visual browser\nncview is a fast and user-friendly visual browser for exploring netCDF files. Mainly designed for Linux and macOS, ncview provides an intuitive interface to quickly view and analyze the contents of any netCDF file, making it a convenient tool for data exploration and preliminary analysis.\n\n\n\n\n\n\ninstalling ncview\n\n\n\n1. Open the Terminal and type:\n\nsudo port install ncview\n\n2. Verify the Installation:\n\nTo verify that ncview is installed, type:\n\n\nncview --version\n\nThis command will display the installed version of ncview\n\n\n\n\n\n\n\n\nncview for Windows?\n\n\n\n\n\nncview is designed for Unix-like environments (Linux and macOS). However, the installation can be achieved by using Windows Subsystem for Linux (WSL):\n1. Install Windows Subsystem for Linux (WSL):\n\nOpen PowerShell as Administrator and run:\n\n\nwsl --install\n\n\nThis command installs WSL along with a default Linux distribution, such as Ubuntu. If you already have WSL, make sure it is set to version 2 by running:\n\n\nwsl --set-default-version 2\n\n2. Install Packages:\n\nOpen the Ubuntu terminal (or your chosen Linux distribution) and update your package list:\n\n\nsudo apt update\n\n\nInstall the required dependencies for ncview:\n\n\nsudo apt install ncview\n\n3. Verify the Installation:\n\nncview\n\n\nThis should start ncview if the installation was successful\n\n4. X server (VcXsrv) for ncview\n\nDownload and install VcXsrv from official GitHub releases page\nLaunch VcXsrv after installation\nSet the DISPLAY Environment Variable in WSL\n\n\nexport DISPLAY=$(grep nameserver /etc/resolv.conf | awk '{print $2}'):0.0"
  },
  {
    "objectID": "05_climate_data.html#working-with-cdo-and-ncview",
    "href": "05_climate_data.html#working-with-cdo-and-ncview",
    "title": "5  Getting Started with Climate Data",
    "section": "5.3 Working with CDO and ncview",
    "text": "5.3 Working with CDO and ncview\nFor the purpose of this workshop and to demonstrate how to use CDO (Climate Data Operators) and ncview, I have included two netCDF files of dynamical fronts (i.e., FSLE - Finite Size Lyapunov Exponent) in the workshop repository you have already cloned. These files are located in the data_raw/2016 directory.\nTo work with CDO and ncview, you’ll need to use the terminal command line. Open the Ubuntu app in Windows or the Terminal on macOS.\n\n\n\n\n\n\nCDO and ncview from the terminal command line\n\n\n\n1.1 Data Directory\n\nIn your command line, type:\n\n\ncd /Users/ibrito/Desktop/NACECC_workshop/data_raw/2016/\n\n# Navigate to the directory where you have stored the workshop repository\n# Note: The path below is specific to my local computer. Please replace it with your own directory path\n\n\nThis command sets NACECC_workshop as your primary working directory\n\n1.2 Windows Users\n\nIf your workshop repository is located on the Desktop, the path should be within the /mnt/c/Users/YourUsername/Desktop/ directory\n\n\ncd /mnt/c/Users/YourUsername/Desktop/NACECC_workshop/data_raw/2016/\n# Replace YourUsername with your actual Windows username.\n\n2. Viewing Data with ncview\n\nCheck if your data file is in the directory by listing the files:\n\n\nls -l\n# This command will display the contents of the data directory\n\n\nTo quickly view the netCDF file using ncview, type:\n\n\nncview dt_global_allsat_madt_fsle_20160601_20210921.nc\n\n\n\n\n\n\n\nIn Windows remember to run first:\n\n\nexport DISPLAY=$(grep nameserver /etc/resolv.conf | awk '{print $2}'):0.0\n\n\nncview dt_global_allsat_madt_fsle_20160601_20210921.nc\n\n3. File Details with CDO\n\nTo inspect the metadata and structure of the netCDF file using CDO, type:\n\n\ncdo -sinfo dt_global_allsat_madt_fsle_20160601_20210921.nc\n# This command provides detailed information about the file, such as variable names, dimensions, and attributes.\n\n\n\n\n\n\nThe model details are:\n\nHorizontal component: resolution 9000x4500 (~4km)\nVertical component: 1 level (i.e., surface)\nTime component: 1 step\n\n4. Check variable(s) name\n\nTo get the variable names from a netCDF file using CDO, you can use the showname function:\n\n\ncdo showname dt_global_allsat_madt_fsle_20160601_20210921.nc\n\nThe model variables are: fsle_max theta_max(Sudre, Hernández-Carrasco, et al. 2023a)\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nX Server for ncview on Windows: If you’re using ncview on Windows, ensure you have an X server (like VcXsrv or Xming) running to properly display the graphical interface\nError Handling: If you encounter any errors, double-check that CDO and ncview are correctly installed and that you are using the correct file paths"
  },
  {
    "objectID": "05_climate_data.html#ocean-front-dataset-aviso-product",
    "href": "05_climate_data.html#ocean-front-dataset-aviso-product",
    "title": "5  Getting Started with Climate Data",
    "section": "5.4 Ocean Front dataset: AVISO product",
    "text": "5.4 Ocean Front dataset: AVISO product\nFor the primary purpose of this workshop, we will use AVISO products, primarily focusing on ocean fronts. AVISO is the French Active Archive Data Center for multi-satellite altimeter missions. It is responsible for post-processing, analyzing, archiving, and distributing altimetry data for CNES (Centre National d’Études Spatiales), the French Space Agency.\nYou can set up an account and start downloading datasets for your research. Visit the website here: https://www.aviso.altimetry.fr\n\n\n\n\n\n\nNote\n\n\n\nI strongly recommend downloading the data via FTP to simplify your workflow. You can use FTP clients such as WinSCP, FileZilla, or MobaXterm.\n\n\n\n\n\n\n\n\n5.4.1 Dynamical fronts: Finite-size lyapunov exponent (FSLE)\nIn simple terms, the Finite-size Lyapunov Exponent (FSLE) is a tool used to understand how the ocean moves and mixes. FSLE helps identify lines in the ocean that act like barriers or pathways. High FSLE values highlight lines that act as transport barriers (known as Lagrangian coherent structures), which help identify features like filaments, fronts, or eddy boundaries.\nMore information on these aspects, please read (Sudre, Dewitte, et al. 2023) and (Sudre, Hernández-Carrasco, et al. 2023b)\n\n\n5.4.2 Procesing FSLE data from AVISO (or any other climate data)\nYou do not actually have to download the front dataset from AVISO at this point. We will use the dataset located in the workshop repository that you have previously cloned.\n\n\n\n\n\n\nNACECC_workshop_repo\n\n\n\n\nGo to the location where you cloned the repository\nOpen the NACECC_workshop_repo.Rproj file\nThis process should open a new R session in RStudio and, by default, set the NACECC_workshop_repo as the working directory, with relative paths\n\n\n\n\n\n\n\n\nThe AVISO dataset on fronts consists of individual daily files of 140 MB each, covering the period from 1994 — 2023. You can imagine that processing these files in R could be quite challenging. R will need to use YOUR local memory just to read these files, and any additional tasks we need to perform (such as aggregating, merging, calculating averages, minimums, maximums, etc.) will also rely on your local computer’s internal memory.\n\n\n\n\n\n\n\n\n\n\n\nExample 1: Using CDO to merge files\n\n\n\nThis is the structure of the NACECC_workshop_repo repository that we have cloned. I did not include all the AVISO files; as an example, we will work with only 2 files. You can repeat the same exercise with however many files you have for your projects.\n\n\n\n\n\nTo better handle front or any other climate data, organizing the dataset by year or month is the way to go. In fact, the IPCC data structure typically works with monthly files (and sometimes yearly files).\nUsing CDO to merge files is simple. We will have to use -mergetime operator in the directory. For example, using the terminal or the RStudio terminal tab:\n\n# Navigate to the directory where the front data is stored\ncd data_raw/2016/\n\n# Check how many NetCDF files are in the directory\nls -l *.nc\n\n# Combine the two NetCDF files\ncdo -mergetime *.nc combined_file.nc\n\n\n\n\n\n\nThe CDO Syntax is very simple. It requieres:\n\ncdo -Operator input_files output_files\n\nFrom the code before we can see:\n\n# Specify the operator for merging files\nOperator == \"-mergetime\" \n\n# Define the input files (using wildcard * to identify all .nc files in the directory)\ninput_files == \"*.nc\" \n\n# Define the output file name (make sure to include the extension)\noutput_files == \"combined_file.nc\"\n\n\n\n\n\n\n\n\n\nExample 2: Check the new file and plot it using ncview\n\n\n\nTo check the structure of the new combined_file.nc file, we will use the -sinfo operator. In the same directorty data_raw/2016/, type (using the terminal or the RStudio terminal tab):\n\ncdo -sinfo combined_file.nc\n\n\n\n\n\n\nTo quickly plot the new combined_file.nc using ncview, type:\n\nncview combined_file.nc"
  },
  {
    "objectID": "05_climate_data.html#exploring-useful-cdo-functions",
    "href": "05_climate_data.html#exploring-useful-cdo-functions",
    "title": "5  Getting Started with Climate Data",
    "section": "5.5 Exploring Useful CDO Functions",
    "text": "5.5 Exploring Useful CDO Functions\nCDO provides a wide range of functions for processing and analyzing climate data in netCDF format. Below are some of the most commonly used functions, along with additional operators that can be useful for specific tasks.\n\n\n\n\n\n\nCDO Functions\n\n\n\n1. Commonly Used\n\ncdo -yearmean: Calculates the annual mean from a monthly data input netCDF file\ncdo -yearmin: Calculates the annual minimum from a monthly data input netCDF file\ncdo -yearmax: Calculates the annual maximum from a monthly data input netCDF file\ncdo -ensmean: Computes the ensemble mean across multiple netCDF files. This is particularly useful when you have input files from - different models and want to calculate the mean across all models\ncdo -vertmean: Calculates the vertical mean for netCDF files with ocean levels (e.g., depth)\ncdo -mergetime: Merges all netCDF files in your directory into a single file, combining data over time\n\n2. Additional Operators\n\ncdo sellonlatbox,170,-170,-10,-30 input_file.nc output_file.nc: Selects a specific region within the data, defined by the longitude and latitude box (in this case, from 170° to -170° longitude and from -10° to -30° latitude)\ncdo -remapbil,r1440x720 -select,name=thetao input_file.nc output_file.nc: Performs bilinear interpolation to remap the data to a 1440x720 grid, selecting the variable thetao (temperature at depth).\ncdo -remapdis,r1440x720 -select,name=thetao input_file.nc output_file.nc: Performs distance-weighted interpolation to remap the data to a 1440x720 grid, selecting the variable thetao.\ncdo -remapbil,r4000x2000 -select,name=fsle input_file.nc output_file.nc: Performs bilinear interpolation to remap the data to a 4000x2000 grid, selecting the variable fsle (Finite-size Lyapunov Exponent)"
  },
  {
    "objectID": "05_climate_data.html#workflow-for-using-climate-model-outputs",
    "href": "05_climate_data.html#workflow-for-using-climate-model-outputs",
    "title": "5  Getting Started with Climate Data",
    "section": "5.6 Workflow for using climate model outputs",
    "text": "5.6 Workflow for using climate model outputs\nFor those seeking to incorporate climate models into their professional work to address climate change, understanding the starting point can be challenging. Here is a framework, detailed in our recently published paper online (Schoeman et al. 2023).\n\n\n\n\n\n\n\n\n\nSchoeman, David S., Alex Sen Gupta, Cheryl S. Harrison, Jason D. Everett, Isaac Brito-Morales, Lee Hannah, Laurent Bopp, Patrick R. Roehrdanz, and Anthony J. Richardson. 2023. “Demystifying Global Climate Models for Use in the Life Sciences.” Trends in Ecology & Evolution 38 (9): 843–58. https://doi.org/10.1016/j.tree.2023.04.005.\n\n\nSudre, Floriane, Boris Dewitte, Camille Mazoyer, Véronique Garçon, Joel Sudre, Pierrick Penven, and Vincent Rossi. 2023. “Spatial and Seasonal Variability of Horizontal Temperature Fronts in the Mozambique Channel for Both Epipelagic and Mesopelagic Realms.” Frontiers in Marine Science 9. https://www.frontiersin.org/articles/10.3389/fmars.2022.1045136.\n\n\nSudre, Floriane, Ismael Hernández-Carrasco, Camille Mazoyer, Joel Sudre, Boris Dewitte, Véronique Garçon, and Vincent Rossi. 2023a. “An Ocean Front Dataset for the Mediterranean Sea and Southwest Indian Ocean.” Scientific Data 10 (1): 730. https://doi.org/10.1038/s41597-023-02615-z.\n\n\n———. 2023b. “An Ocean Front Dataset for the Mediterranean Sea and Southwest Indian Ocean.” Scientific Data 10 (1): 730. https://doi.org/10.1038/s41597-023-02615-z."
  },
  {
    "objectID": "06_ncRaster.html#data-import",
    "href": "06_ncRaster.html#data-import",
    "title": "6  netCDF files in R: Raster, Spatial objects",
    "section": "6.1 Data import",
    "text": "6.1 Data import\nTo run the next chunks of code, you’ll need a few R packages. We can approach this in two ways: the usual way, which is straightforward, or a more advanced method if you’re feeling like an expert!\n\ninstall.packages(c(\"terra\", \"ncdf4\", \"ncdf4.helpers\", \"PCICt\", \n                   \"dplyr\", \"magrittr\"))\n\n# load packages\n  library(terra)\n  library(ncdf4)\n  library(ncdf4.helpers)\n  library(PCICt)\n  library(dplyr)\n  library(magrittr)\n\n# List of pacakges that we will use\n  list.of.packages &lt;- c(\"terra\", \"ncdf4\", \"ncdf4.helpers\", \"PCICt\", \"dplyr\", \"magrittr\")\n\n# If is not installed, install the package\n  new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n      if(length(new.packages)) install.packages(new.packages)\n\n# Load packages\n  lapply(list.of.packages, require, character.only = TRUE)"
  },
  {
    "objectID": "06_ncRaster.html#function-to-transform-netcdf-files-into-raster-objects",
    "href": "06_ncRaster.html#function-to-transform-netcdf-files-into-raster-objects",
    "title": "6  netCDF files in R: Raster, Spatial objects",
    "section": "6.2 Function to transform netCDF files into Raster objects",
    "text": "6.2 Function to transform netCDF files into Raster objects\nThis code provides enhanced control over the outputs compared to standard methods. By handling the netCDF data step by step, it ensures that the spatial and time dimensions are correctly aligned, making the resulting raster stack ready for any analysis you want to do. This is especially useful when dealing with larger or more complicated datasets, where a bit of extra care in setting things up can make a big difference in the accuracy and performance of your work.\nLet’s walk through the code step by step.\n\n# Define general variables\nnc = \"data_raw/2016/combined_file.nc\"  # Path to the netCDF file\nv = \"fsle_max\"  # Variable name in the netCDF file to be extracted\nx = \"lon\"  # Longitude variable\ny = \"lat\"  # Latitude variable\n\n# Open the netCDF file\nnc &lt;- ncdf4::nc_open(nc)  # Load the netCDF file\ndat &lt;- ncdf4::ncvar_get(nc, v)  # Extract the data for the specified variable (v)\ndat[] &lt;- dat  # Store the data in a matrix for further processing\n\n# Get the range of longitude and latitude values\nrlon &lt;- ncdf4::ncvar_get(nc, varid = x) %&gt;% range()  # Longitude range\nrlat &lt;- ncdf4::ncvar_get(nc, varid = y) %&gt;% range()  # Latitude range\n\n# Determine the dimensions of the data (X = number of longitude points, Y = number of latitude points)\nX &lt;- dim(dat)[1]\nY &lt;- dim(dat)[2]\n\n# Extract and format the time dimension as a Date vector\ntt &lt;- ncdf4.helpers::nc.get.time.series(nc, v = \"time\", time.dim.name = \"time\")\ntt &lt;- as.POSIXct(tt) %&gt;% as.Date()  # Convert to Date format\n\n# Close the netCDF file to free up resources\nncdf4::nc_close(nc)\n\n# Create a raster template with the appropriate dimensions and extent\nrs &lt;- terra::rast(nrow = Y, ncol = X, extent = terra::ext(c(rlon, rlat)))\n\n# Fix the orientation of the data to match the raster orientation\ndrs &lt;- terra::xyFromCell(rs, 1:terra::ncell(rs)) %&gt;% as_tibble()\n\n# Initialize an empty list to store the raster layers\nrs_list &lt;- list()  \nst &lt;- terra::rast()  # Initialize an empty raster stack\n\n# Loop through each time step, creating a raster layer for each\nfor (i in 1:length(tt)) {  \n  dt1 &lt;- dplyr::bind_cols(drs, as.vector(dat[, , i])) %&gt;%\n    magrittr::set_colnames(c(\"x\", \"y\", v))  # Bind coordinates and data into a single table\n  \n  dt1 &lt;- terra::rast(dt1, type = \"xyz\")  # Create a raster from the table\n  names(dt1) &lt;- tt[i]  # Name the raster layer based on the date\n  \n  st &lt;- c(st, terra::flip(dt1))  # Add the raster layer to the stack after flipping orientation if necessary\n  \n  print(paste0(tt[i], \" of \", length(tt)))  # Print progress information\n}\n\n\n\n\n\n\n\nWarning\n\n\n\nThe code above is designed to work with netCDF files that have more than one time layer, assuming you’re dealing with complex netCDF files. However, it can be easily adapted for simpler files (1 time layer) by replacing the loop above with the following code:\n\n# For simpler netCDF files, you can remove the loop above and use this code instead:\nfor (i in 1:length(tt)) {  # Loop through each time step\n  dt1 &lt;- dplyr::bind_cols(drs, as.vector(dat[])) %&gt;%  # Combine coordinates with data (no need to loop through data)\n    magrittr::set_colnames(c(\"x\", \"y\", v))  # Set column names\n  \n  dt1 &lt;- terra::rast(dt1, type = \"xyz\")  # Create a raster from the table\n  names(dt1) &lt;- tt[i]  # Assign the time step as the raster name\n  st &lt;- c(st, terra::flip(dt1))  # Add the raster to the stack after flipping its orientation\n  \n  print(paste0(tt[i], \" of \", length(tt)))  # Print progress\n}"
  },
  {
    "objectID": "06_ncRaster.html#cropmanipulate-and-project",
    "href": "06_ncRaster.html#cropmanipulate-and-project",
    "title": "6  netCDF files in R: Raster, Spatial objects",
    "section": "6.3 Crop/Manipulate and project",
    "text": "6.3 Crop/Manipulate and project\n\n# Define the output directory\noutdir &lt;- \"data_rout/\"\n\n# Load the raster file\nrs  &lt;-  \"data_raw/2016/combined_file.nc\"\nrs01 &lt;- rast(rs)\n\n# Rotate the raster to correct orientation issues (if any)\nrs02 &lt;- terra::rotate(rs01)\n\n# Crop the raster to the specified extent (coordinates for the North Atlantic region)\nrs03 &lt;- terra::crop(rs02, ext(-70, -10, 33, 67))\n\n# Generate the output file name by appending the input file name to the output directory\noutput_name &lt;- paste0(outdir, basename(rs))\n\n# Save the cropped raster to the specified output directory\nterra::writeRaster(rs03, output_name, overwrite = TRUE)\n\n\n\n\n\n\n\nAutomated crop Function for Multiple Files\n\n\n\n\n\n\nrs_split &lt;- function(rs, outdir) {\n  \n  library(dplyr)\n  library(terra)\n  library(stringr)\n  \n  dir_rs &lt;- list.files(path = rs, \n                       pattern = \"*.tif\", \n                       all.files = TRUE, \n                       full.names = TRUE, \n                       recursive = FALSE)\n  FF &lt;- lapply(dir_rs, function(x){\n    rs01 &lt;- rast(x)\n    rs02 &lt;- terra::rotate(rs01)\n    rs03 &lt;- terra::crop(rs02, ext(-70, -10, 33, 67)) # North Atlantic\n  # Generate output file name based on input file name\n    # output_name &lt;- gsub(pattern = \"\\\\.tif$\", replacement = \"_processed.tif\", basename(x))\n    output_name &lt;- paste0(outdir, basename(x))\n    terra::writeRaster(rs03, output_name)\n  })\n}\n\nrs_split(rs = \"data_raw/fsle_rs00\",\n         outdir = \"data_raw/fsle_rs/\")"
  },
  {
    "objectID": "06_ncRaster.html#save-the-raster-file",
    "href": "06_ncRaster.html#save-the-raster-file",
    "title": "6  netCDF files in R: Raster, Spatial objects",
    "section": "6.4 Save the Raster file",
    "text": "6.4 Save the Raster file\n\n# Writing Raster\n  terra::writeRaster(st,\n                     \"tos_Omon_GFDL-ESM4_ssp585_r1i1p1f1_gr_201501-203412.tif\",\n                     overwrite = TRUE,\n                     filetype = \"GTiff\")"
  },
  {
    "objectID": "07_equalGrid.html#data-import",
    "href": "07_equalGrid.html#data-import",
    "title": "7  equal-area grid for your area of interest",
    "section": "7.1 Data import",
    "text": "7.1 Data import\nLoad the required packages and some files from the z_helpFX.R script.\n\nsource(\"zscripts/z_helpFX.R\")"
  },
  {
    "objectID": "07_equalGrid.html#defining-the-spatial-limits",
    "href": "07_equalGrid.html#defining-the-spatial-limits",
    "title": "7  equal-area grid for your area of interest",
    "section": "7.2 Defining the spatial limits",
    "text": "7.2 Defining the spatial limits\n\n\n\n\n\n\nbounding box\n\n\n\nThe bounding box defined here is not for the entire North Atlantic, but specifically for a subset of the seabird dataset that we will use as an example in this workshop. You can find the seabird dataset in the inputs_sb folder within the workshop repository that you have already cloned.\n\n# Define the bounding box for the area of interest\n# Coordinates correspond to the specified region (xmin, xmax, ymin, ymax)\n# The CRS is set to match the existing LatLon CRS\nbbox &lt;- st_bbox(c(xmin = -54, xmax = -35, ymin = 40, ymax = 57), \n                crs = st_crs(LatLon)) %&gt;% \n  st_as_sfc() %&gt;%  # Convert the bounding box to a simple feature geometry (sfc)\n  st_transform(crs = robin)  # Transform the CRS to the Robinson projection\nf_bbox &lt;- bbox  # Assign the transformed bbox to f_bbox for further use\n\n# Define the cell area for the grid (in square kilometers)\nCellArea &lt;- 4 # Set the area for each grid cell to 4 km²\n\n# Calculate the diameter of a hexagonal grid cell in meters\nh_diameter &lt;- 2 * sqrt((CellArea*1e6)/((3*sqrt(3)/2))) * sqrt(3)/2 \n\n# Alternatively, calculate the side length of a square grid cell in meters\ns_diameter &lt;- sqrt(CellArea*1e6)"
  },
  {
    "objectID": "07_equalGrid.html#creating-an-equal-area-grid",
    "href": "07_equalGrid.html#creating-an-equal-area-grid",
    "title": "7  equal-area grid for your area of interest",
    "section": "7.3 Creating an equal-area grid",
    "text": "7.3 Creating an equal-area grid\n\n\n\n\n\n\nspatial grid\n\n\n\n\n# Create the grid (planning units) for the entire region based on the bounding box\n# The grid cells are hexagonal (square = F) with the specified cell size (h_diameter)\n# The grid is created as polygons and uses the CRS of the bounding box\nPUs &lt;- st_make_grid(f_bbox,\n                    square = F,\n                    cellsize = c(h_diameter, h_diameter),\n                    what = \"polygons\",\n                    crs = st_crs(f_bbox)) %&gt;%\n  st_sf()  # Convert the grid to a simple feature (sf) object\n\n# Verify the size of the grid cells to ensure they are correct\n# Calculate the area of each grid cell and convert the units to km²\n# Print the range of cell sizes to check for consistency\nprint(paste0(\"Range of cellsize are \",\n             round(as.numeric(range(units::set_units(st_area(PUs), \"km^2\")))[1]),\" km² to \",\n             round(as.numeric(range(units::set_units(st_area(PUs), \"km^2\")))[2]),\" km²\"))\n\n# Identify grid cells that do not intersect with the North Atlantic region\nlogi_PUs &lt;- st_centroid(PUs) %&gt;%\n  st_intersects(naSF_rob) %&gt;% \n  lengths &gt; 0  # Convert to a logical vector indicating intersections\n\n# Filter out grid cells that intersect with the North Atlantic region, leaving only relevant cells\nPUs1 &lt;- PUs[logi_PUs == FALSE, ]"
  },
  {
    "objectID": "07_equalGrid.html#plotting-the-output",
    "href": "07_equalGrid.html#plotting-the-output",
    "title": "7  equal-area grid for your area of interest",
    "section": "7.4 Plotting the output",
    "text": "7.4 Plotting the output\n\n\n\n\n\n\nggplot()\n\n\n\n\ng1 &lt;- ggplot() +\n    geom_sf(data = PUs, size = 0.05) +\n    geom_sf(data = worldsf_rob, size = 0.05, fill = \"grey20\") +\n    theme_bw() +\n    coord_sf(xlim = c(st_bbox(worldsf_rob2)$xmin + 85000, st_bbox(worldsf_rob2)$xmax - 85000),\n             ylim = c(st_bbox(worldsf_rob2)$ymin + 70000, st_bbox(worldsf_rob2)$ymax - 70000),\n             expand = TRUE)\n print(g1)\n\n\n\n\n\n\n\n\n\nWriting the grid\n\n\n\nYou can create the new grid file by typing the following command. However, the workshop repository for this eBook already contains the file, so you may want to change the directory path below before running the code in the RStudio console.\n\nst_write(obj = PUs1, dsn = \"input_layers/boundaries/\", layer = \"PUs_NA_04km2\", driver = \"ESRI Shapefile\")\n\nThe same applies to the previous ggplot we created.\n\nggsave(\"input_layers/boundaries/PUs_NA_04km2.png\", plot = g1, width = 30, height = 30, dpi = 300, limitsize = FALSE)\n\n\n\n\n\n\n\n\n\nz_fx_punits.R function in zscripts directory\n\n\n\n\n\nThe entire function above is located in the zscripts directory. You can also find it below if you want to paste it into a new script in your RStudio console.\n\n# Define the bounding box for the area of interest\n# Coordinates correspond to the specified region (xmin, xmax, ymin, ymax)\n# The CRS is set to match the existing LatLon CRS\nbbox &lt;- st_bbox(c(xmin = -54, xmax = -35, ymin = 40, ymax = 57), \n                crs = st_crs(LatLon)) %&gt;% \n  st_as_sfc() %&gt;%  # Convert the bounding box to a simple feature geometry (sfc)\n  st_transform(crs = robin)  # Transform the CRS to the Robinson projection\nf_bbox &lt;- bbox  # Assign the transformed bbox to f_bbox for further use\n\n# Define the cell area for the grid (in square kilometers)\nCellArea &lt;- 4 # Set the area for each grid cell to 4 km²\n\n# Calculate the diameter of a hexagonal grid cell in meters\nh_diameter &lt;- 2 * sqrt((CellArea*1e6)/((3*sqrt(3)/2))) * sqrt(3)/2 \n\n# Alternatively, calculate the side length of a square grid cell in meters\ns_diameter &lt;- sqrt(CellArea*1e6)\n\n# Create the grid (planning units) for the entire region based on the bounding box\n# The grid cells are hexagonal (square = F) with the specified cell size (h_diameter)\n# The grid is created as polygons and uses the CRS of the bounding box\nPUs &lt;- st_make_grid(f_bbox,\n                    square = F,\n                    cellsize = c(h_diameter, h_diameter),\n                    what = \"polygons\",\n                    crs = st_crs(f_bbox)) %&gt;%\n  st_sf()  # Convert the grid to a simple feature (sf) object\n\n# Verify the size of the grid cells to ensure they are correct\n# Calculate the area of each grid cell and convert the units to km²\n# Print the range of cell sizes to check for consistency\nprint(paste0(\"Range of cellsize are \",\n             round(as.numeric(range(units::set_units(st_area(PUs), \"km^2\")))[1]),\" km² to \",\n             round(as.numeric(range(units::set_units(st_area(PUs), \"km^2\")))[2]),\" km²\"))\n\n# Identify grid cells that do not intersect with the North Atlantic region\nlogi_PUs &lt;- st_centroid(PUs) %&gt;%\n  st_intersects(naSF_rob) %&gt;% \n  lengths &gt; 0  # Convert to a logical vector indicating intersections\n\n# Filter out grid cells that intersect with the North Atlantic region, leaving only relevant cells\nPUs1 &lt;- PUs[logi_PUs == FALSE, ]"
  },
  {
    "objectID": "08_matchingMFOFC.html#data-import",
    "href": "08_matchingMFOFC.html#data-import",
    "title": "8  Extracting Front Data (or any variable) for the Created Grid",
    "section": "8.1 Data import",
    "text": "8.1 Data import\nSource the required helper.\n\nsource(\"zscripts/z_helpFX.R\") # and the Help function just in case :-)"
  },
  {
    "objectID": "08_matchingMFOFC.html#a-helper-function-to-replace-na-with-nearest-neighbor-values",
    "href": "08_matchingMFOFC.html#a-helper-function-to-replace-na-with-nearest-neighbor-values",
    "title": "8  Extracting Front Data (or any variable) for the Created Grid",
    "section": "8.2 A helper function to replace NA with nearest neighbor values",
    "text": "8.2 A helper function to replace NA with nearest neighbor values\nBefore proceeding, we need to define a helper function to replace NA values with the nearest neighbor’s values. This is necessary because when interpolating non-equal data, such as a raster, onto an equal-area grid, you may encounter NA values during the overlapping process. These NA values typically arise because the interpolation does not have sufficient data points in certain areas, leading to gaps in the resulting grid.\nWhen performing spatial operations like calculating a weighted mean, R may not be able to handle these NA values correctly, which can result in inaccurate or incomplete results. By replacing NA values with the nearest neighbor’s values, we ensure that the entire grid is populated with meaningful data, allowing for accurate calculations and analyses.\n\n\n\n\n\n\nreplace NAs function (written by Jason Everett)\n\n\n\n\n\n\nfCheckNAs &lt;- function(df, vari) {\n    if (sum(is.na(pull(df, !!sym(vari)))) &gt; 0) { # Check if there are NAs in the specified variable\n      \n      gp &lt;- df %&gt;%\n        mutate(isna = is.finite(!!sym(vari))) %&gt;%\n        group_by(isna) %&gt;%\n        group_split()  # Split the data frame into two groups: with and without NAs\n      \n      out_na &lt;- gp[[1]]    # DataFrame with NAs\n      out_finite &lt;- gp[[2]] # DataFrame without NAs\n      \n      d &lt;- st_nn(out_na, out_finite) %&gt;% # Find the nearest neighbor for each NA value\n        unlist()\n      \n      out_na &lt;- out_na %&gt;%\n        mutate(!!sym(vari) := pull(out_finite, !!sym(vari))[d]) # Replace NAs with the nearest neighbor values\n      \n      df &lt;- rbind(out_finite, out_na) # Combine the data frames back together\n    }\n    return(df)\n  }"
  },
  {
    "objectID": "08_matchingMFOFC.html#read-extract-and-weighted-mean-interpolation",
    "href": "08_matchingMFOFC.html#read-extract-and-weighted-mean-interpolation",
    "title": "8  Extracting Front Data (or any variable) for the Created Grid",
    "section": "8.3 Read, Extract, and Weighted Mean Interpolation",
    "text": "8.3 Read, Extract, and Weighted Mean Interpolation\nThis code is designed to process spatial data by working with a raster dataset and a shapefile that defines specific areas of interest. It aligns both the raster data and the shapefile to the same map projection. Then, it calculates the average values of the raster data within each defined area, accounting for differences in cell size. If any data points are missing, the code fills in those gaps using the nearest available data.\nThe code is also designed to run on multiple processor cores simultaneously, making it faster and more efficient, especially when dealing with large datasets.\n\n\n\n\n\n\nWarning\n\n\n\nThis code may take some time to process. For the sake of this workshop, you can skip this step as the necessary files have already been provided in data_rout/fsle_rds.\n\n\n\n# Define paths to the input and output directories, and the desired projection\nrs_path = \"data_raw/fsle_rs\"  # Directory where the raw raster files are stored\nshp_path = \"input_layers/boundaries/PUs_NA_04km2.shp\"  # Path to the shapefile representing planning units\noutdir = \"data_rout/\"  # Directory where output files will be saved\nproj.geo = \"ESRI:54030\"  # Projection to be used for transforming spatial data\n\n# Get only the first .tif raster file in the specified directory as an example\nrs_fsel &lt;- list.files(path = rs_path, pattern = \".tif\", full.names = TRUE) [1]\n\n# Loop through each raster file in the list\nfor (j in seq_along(rs_fsel)) {\n  \n  # Step 1: Read the shapefile and transform it to the desired projection\n  shp_file &lt;- st_read(shp_path) %&gt;% \n    st_transform(crs = terra::crs(proj.geo))\n  \n  # Step 2: Read the raster file and set its CRS to EPSG:4326 (WGS 84)\n  rs_file &lt;- rast(rs_fsel[j])\n  crs(rs_file) &lt;- terra::crs(\"EPSG:4326\")\n  \n  # Step 3: Calculate the cell size (area) of the raster and reproject both the raster and its weights to the desired CRS\n  weight_rs &lt;- terra::cellSize(rs_file)  # Calculate the cell size (area) of the raster\n  rs_file &lt;- terra::project(rs_file, y = terra::crs(proj.geo), method = \"near\")  # Reproject the raster\n  weight_rs &lt;- terra::project(weight_rs, y = terra::crs(proj.geo), method = \"near\")  # Reproject the cell size raster\n  \n  # Step 4: Rename the raster layers if there are duplicated names to avoid conflicts\n  if (sum(duplicated(names(rs_file))) != 0) {\n    names(rs_file) &lt;- seq(from = as.Date(names(rs_file)[1]), \n                          to = as.Date(paste0(paste0(unlist(stringr::str_split(names(rs_file)[1], \"-\"))[1:2], collapse = \"-\"), \"-\",length(names(rs_file)))), \n                          by = \"day\")\n  } else {\n    rs_file\n  }\n  \n  # Step 5: Extract raster values for each planning unit (polygon) using weighted mean\n  rs_bypu &lt;- exact_extract(rs_file, \n                           shp_file, \n                           \"weighted_mean\", \n                           weights = weight_rs, \n                           append_cols = TRUE, \n                           full_colnames = TRUE)\n  \n  # Step 6: Join the extracted values back to the shapefile based on the FID (unique identifier)\n  rs_shp &lt;- dplyr::right_join(shp_file, rs_bypu, \"FID\")\n  \n  # Step 7: Clean up column names by removing the \"weighted_mean.\" prefix\n  colnames(rs_shp) &lt;- stringr::str_remove_all(string = names(rs_shp), pattern = \"weighted_mean.\")\n  \n  # Step 8: Set up parallel processing for handling large data sets\n  cores  &lt;-  detectCores() -1  # Define the number of cores to use for parallel processing\n  cl &lt;- makeCluster(cores)  # Create a cluster object for parallel processing\n  registerDoParallel(cl)  # Register the cluster for use in parallel processing\n  \n  # Step 9: Prepare to process each column of the data frame in parallel\n  nms &lt;- names(rs_shp)  # Get the column names\n  nms &lt;- nms[nms != \"geometry\" & nms != \"FID\"]  # Exclude geometry and FID columns from processing\n  ls_df &lt;- vector(\"list\", length = length(nms))  # Initialize a list to store results\n  \n  # Step 10: Process each variable in parallel using foreach loop\n  df1 &lt;- foreach(i = 1:length(nms), .packages = c(\"terra\", \"dplyr\", \"sf\", \"exactextractr\", \"nngeo\", \"stringr\")) %dopar% {\n    single &lt;- rs_shp %&gt;% \n      dplyr::select(FID, nms[i])  # Select the FID and the current variable\n    rs_sfInt &lt;- fCheckNAs(df = single, vari = names(single)[2]) %&gt;%  # Apply the helper function to handle NAs\n      as_tibble() %&gt;% \n      dplyr::arrange(FID) %&gt;%\n      dplyr::select(-FID, -geometry)  # Remove the FID and geometry columns\n    ls_df[[i]] &lt;- rs_sfInt  # Store the processed data in the list\n  }\n  \n  # Step 11: Stop the parallel processing cluster\n  stopCluster(cl)\n  \n  # Step 12: Combine all the processed data into a single data frame\n  rs_sfInt &lt;- do.call(cbind, df1) %&gt;%\n    as_tibble()\n  \n  # Step 13: Generate a filename for the output based on the raster file's name\n  ns &lt;- stringr::str_remove_all(basename(rs_fsel[j]), pattern = \".tif\")\n  \n  # Step 14: Save the processed data as an RDS file in the specified output directory\n  saveRDS(rs_sfInt, paste(outdir, ns, \".rds\", sep = \"\"))\n}"
  },
  {
    "objectID": "08_matchingMFOFC.html#plot-the-output",
    "href": "08_matchingMFOFC.html#plot-the-output",
    "title": "8  Extracting Front Data (or any variable) for the Created Grid",
    "section": "8.4 Plot the output",
    "text": "8.4 Plot the output\nPlotting the output is straightforward. Just follow these steps in the note below:\n\n\n\n\n\n\nReading and ploting the FSLE data\n\n\n\n1. Load and Prepare Data\n\npus &lt;- st_read(\"input_layers/boundaries/PUs_NA_04km2.shp\") \nfsle &lt;- readRDS(\"data_rout/fsle_rds/dt_global_allsat_madt_fsle_2016-06.rds\") %&gt;% \n  dplyr::mutate(across(everything(), ~ .x * -1))\nDFfsle &lt;- cbind(pus, fsle$`2016-06-01.area`)\n\nIn this step, we load the planning unit shapefile and the FSLE data for June 2016. We also adjust the FSLE values and combine them with the shapefile data.\n2. Create the Plot\n\np1 &lt;- ggplot() +\n  geom_sf(data = DFfsle, aes(fill = fsle..2016.06.01.area.), colour = NA) +\n  scale_fill_distiller(palette = \"Spectral\",\n                       direction = -1,\n                       oob = scales::squish,\n                       guide = guide_colourbar(title.position = \"top\", \n                                               title = expression(\"Ocean Fronts (FSLE: day\" * \" \"^{-1} * \")\"))) +\n  geom_sf(data = worldsf_rob, size = 0.05, fill = \"grey20\") +\n  theme_bw() +\n  coord_sf(xlim = c(st_bbox(worldsf_rob2)$xmin + 85000, st_bbox(worldsf_rob2)$xmax - 85000),\n           ylim = c(st_bbox(worldsf_rob2)$ymin + 70000, st_bbox(worldsf_rob2)$ymax - 70000),\n           expand = TRUE) +\n  theme(plot.title = element_text(face = \"plain\", size = 20, hjust = 0.5),\n        plot.tag = element_text(colour = \"plain\", face = \"bold\", size = 23), \n        axis.title.y = element_blank(),\n        axis.title.x = element_blank(),\n        axis.text.x = element_text(size = rel(2), angle = 0),\n        axis.text.y = element_text(size = rel(2), angle = 0),\n        legend.title = element_text(colour = \"black\", face = \"bold\", size = 15),\n        legend.text = element_text(colour = \"black\", face = \"bold\", size = 13),\n        legend.key.height = unit(1.5, \"cm\"),\n        legend.key.width = unit(1.5, \"cm\"))\n\nHere, we create a plot using ggplot2 R package. The plot shows the FSLE data with a color scale representing ocean fronts. We also add geographic boundaries and customize the plot’s appearance.\n\n\n\n\n\n\n\n\n\n3. Save the Plot:\n\nggsave(\"figures/dt_global_allsat_madt_fsle_2016-06.png\", plot = p1, width = 20, height = 15, dpi = 350, limitsize = FALSE)\n\nFinally, we save the plot as a high-resolution PNG file, specifying the dimensions and quality settings."
  },
  {
    "objectID": "09_fronts_megafauna.html#seabird-dataset",
    "href": "09_fronts_megafauna.html#seabird-dataset",
    "title": "9  Megafauna and Ocean Fronts",
    "section": "9.1 Seabird dataset",
    "text": "9.1 Seabird dataset\nIn this chapter, we will explore whether marine megafauna intersect with the ocean front dataset we generated earlier. We will use the Gull Island dataset, which contains tracking data from 2016-2022, with positions recorded every two hours. Each bird in the dataset has a unique birdID, and multiple trips are recorded for each bird, each with its own unique ID.\nThe dataset includes a states3_hr column that categorizes behavior into three states:\n\ntransit: rapid movement to reach foraging areas\nextensive search: slower movement with tighter turns, likely searching for food\nintensive search very slow movement with tight turns, indicating foraging in dense prey patches\n\nThere is a states2from3 column that combines the extensive and intensive search states into a single foraging category. This will help us determine how the movements of marine megafauna align with the ocean fronts we’ve identified.\nFor more information about the dataset, contact April.Hedd@ec.gc.ca and Katharine Studholme Katharine.Studholme@ec.gc.ca."
  },
  {
    "objectID": "09_fronts_megafauna.html#data-import",
    "href": "09_fronts_megafauna.html#data-import",
    "title": "9  Megafauna and Ocean Fronts",
    "section": "9.2 Data import",
    "text": "9.2 Data import\nTo make the most of our time during this workshop, we will focus on using data from 2021, specifically the records from July and August. You can source the necessary helper functions with the script z_helpFX.R and load the relevant files using z_inputFls_local.R.\n\n# Source the helper functions and input files\n\nsource(\"zscripts/z_helpFX.R\")  # Load helper functions from the script\nsource(\"zscripts/z_inputFls_local.R\")  # Load input files specific to this workshop\n\nAlternatively, you can filter any date within the dataset by using the following code:\n\n\n\n\n\n\nfiltering the seabird dataset\n\n\n\n\n# Load necessary libraries\nlibrary(dplyr)  # For data manipulation\nlibrary(readr)  # For reading and writing data\n\n# Step 1: Load the general seabird dataset and convert it to a tibble for easier manipulation\nsbG &lt;- readRDS(\"data_raw/seabird_test/data_HMMclassified_Gull_2016_22.rds\") %&gt;% \n  as_tibble()\n\n# Step 2: Filter foraging behavior for July 2021\n# This filters the dataset for entries in July 2021 where the seabird behavior is classified as 'Foraging'\nsbF_07 &lt;- sbG %&gt;% \n  dplyr::filter(datetime_utc %in% sbG$datetime_utc[stringr::str_detect(string = sbG$datetime_utc, pattern = \"2021-07.*\")]) %&gt;% \n  dplyr::filter(states2from3 == \"Foraging\")\n\n# Similar to the above, but filtering for August 2021\nsbF_08 &lt;- sbG %&gt;% \n  dplyr::filter(datetime_utc %in% sbG$datetime_utc[stringr::str_detect(string = sbG$datetime_utc, pattern = \"2021-08.*\")]) %&gt;% \n  dplyr::filter(states2from3 == \"Foraging\")\n\n# Step 3: Filter transit behavior for July 2021\n# This filters the dataset for entries in July 2021 where the seabird behavior is classified as 'Transit'\nsbT_07 &lt;- sbG %&gt;% \n  dplyr::filter(datetime_utc %in% sbG$datetime_utc[stringr::str_detect(string = sbG$datetime_utc, pattern = \"2021-07.*\")]) %&gt;% \n  dplyr::filter(states2from3 == \"Transit\")\n\n# Similar to the above, but filtering for August 2021\nsbT_08 &lt;- sbG %&gt;% \n  dplyr::filter(datetime_utc %in% sbG$datetime_utc[stringr::str_detect(string = sbG$datetime_utc, pattern = \"2021-08.*\")]) %&gt;% \n  dplyr::filter(states2from3 == \"Transit\")\n\n\n\n\n\n\n\n\n\nPlot the data using ggplot\n\n\n\n\n# Source the helper functions and input files\n\nsource(\"zscripts/z_helpFX.R\")  # Load helper functions from the script\nsource(\"zscripts/z_inputFls_local.R\")  # Load input files specific to this workshop\n\n# Convert the filtered seabird data (sbF_07) into a spatial object (sf) using the x and y coordinates\n# The coordinate reference system (CRS) is set to LatLon (likely WGS 84)\nmmF &lt;- sbF_07 %&gt;% \n  sf::st_as_sf(coords = c(\"x\", \"y\"), crs = LatLon) %&gt;% \n  st_transform(crs = robin)  # Transform the spatial data to the Robinson projection (crs = robin)\n\n# Create a ggplot object to visualize the seabird data (mmF) on a map\nmmF_plot &lt;- ggplot() +\n  geom_sf(data = mmF, colour = \"blue\", size = 0.3) +  # Plot the seabird data as blue points\n  geom_sf(data = worldsf_rob, size = 0.05, fill = \"grey20\") +  # Overlay a map of the world with dark grey landmasses\n  theme_bw() +  # Apply a clean, white background theme.\n  coord_sf(xlim = c(st_bbox(worldsf_rob2)$xmin + 85000, st_bbox(worldsf_rob2)$xmax - 85000),\n           ylim = c(st_bbox(worldsf_rob2)$ymin + 70000, st_bbox(worldsf_rob2)$ymax - 70000),\n           expand = TRUE)  # Set the coordinate limits to zoom in on a specific region of the map\n           \n# Display the plot\nprint(mmF_plot)"
  },
  {
    "objectID": "09_fronts_megafauna.html#distance-to-fronts",
    "href": "09_fronts_megafauna.html#distance-to-fronts",
    "title": "9  Megafauna and Ocean Fronts",
    "section": "9.3 Distance to Fronts",
    "text": "9.3 Distance to Fronts\n\n9.3.1 Split the Seabird Dataset by birdID\nThe workshop repository for this eBook includes an inputs_sb directory, which contains two subdirectories: sb_LSP_F and sb_LSP_T. These subdirectories hold individual files for each birdID. Splitting the data into individual files offers the advantage of better control and visualization of each bird’s tracking data, making the workflow more efficient. However, this approach does result in a large number of files.\n\n\n\n\n\n\nsplitting function: f02_SplitGroups_v01.R located in zscripts\n\n\n\n\n\n\n# Source the script that loads input files specific to this workflow\nsource(\"zscripts/z_inputFls_local.R\")\n\n# Define the function f_split that takes a spatial dataset (sps) and an output directory (outdir) as inputs\nf_split &lt;- function(sps, outdir) {\n\n  # Load necessary libraries\n  library(sf)        # For handling spatial data\n  library(terra)     # For raster and vector data operations\n  library(raster)    # For raster data manipulation\n  library(lubridate) # For working with date-time data\n  \n  # Create a copy of the input dataset\n  sps1 &lt;- sps\n  \n  # Extract unique bird IDs and behavior states from the dataset\n  grp &lt;- unique(sps1$birdID)\n  grp2 &lt;- unique(sps1$states2from3)\n  \n  # Double for loop to process each bird ID individually\n  for(j in seq_along(grp)) {\n    \n    # Filter the dataset for the current bird ID\n    grp_sgl &lt;- grp[j]\n    df1 &lt;- sps1 %&gt;% \n      dplyr::filter(birdID == grp_sgl) %&gt;%  # Keep only data for the current bird\n      dplyr::mutate(date = date(datetime_utc)) %&gt;%  # Extract and add a date column\n      dplyr::select(x, y, birdID, datetime_utc, date, states3_hr, states2from3)  # Select relevant columns\n    \n    # Generate a file name for the output file\n    ngrd &lt;- unlist(stringr::str_split(basename(outdir), \"_\"))[2]  # Extract a part of the directory name\n    ndate &lt;- paste0(unlist(stringr::str_split(unlist(unique(df1$date)[1]), \"-\"))[1:2], collapse = \"-\")  # Format the date\n    ffname &lt;- paste(ngrd, gsub(\" \", \"\", grp2), ndate, gsub(\"[./]\", \"\", grp_sgl), sep = \"_\")  # Combine parts to form the file name\n    \n    # Save the filtered data as an RDS file with the generated file name\n    saveRDS(df1, paste0(outdir, ffname, \".rds\"))\n  }\n}\n\n# To run the f_split function on the dataset sbT_07 and save the results in the specified directory\nsystem.time(frw &lt;- f_split(sps = sbT_07,\n                           outdir = \"inputs_sb/sb_LSP_T/\"))\n\n\n\n\nThe function above will create the files located in the inputs_sb directory.\n\n\n9.3.2 Nearest distance to Ocean Fronts\n\n\n\n\n\n\nWarning\n\n\n\nThe following sets of code may take some time to process. For the sake of this workshop, you can skip this step as the necessary files have already been provided in outputs_sb.\n\n\n\n\n\n\n\n\ndistance function\n\n\n\nThis is a general function that will calculate the distance of every megafauna record to each ocean front in the grid we created. We will use this function below to iterate over several dates. Always remember to source the helper functions and input files before running the code.\n\n# Source the helper functions and input files\n\nsource(\"zscripts/z_helpFX.R\")  # Load helper functions from the script\nsource(\"zscripts/z_inputFls_local.R\")  # Load input files specific to this workshop\n\nAnd the function dist_fx:\n\ndist_fx &lt;- function(PUs, fauna, ofdates, cutoff) {\n    \n    UseCores &lt;- 5 # TO 5 IN LOCAL\n    cl &lt;- parallel::makeCluster(UseCores)\n    doParallel::registerDoParallel(cl)\n    lsout &lt;- vector(\"list\", length = nrow(fauna))\n    ls_out &lt;- foreach(x = 1:nrow(fauna),\n                      .packages = c(\"terra\",\n                                    \"dplyr\", \n                                    \"sf\", \n                                    \"stringr\")) %dopar% {\n                                      dist02 &lt;- st_distance(fauna[x, ], ofdates, by_element = FALSE) %&gt;%\n                                        t() %&gt;%\n                                        as_tibble()\n                                      # Get the upper front quantile of front\n                                      qfront &lt;- ofdates %&gt;%\n                                        as_tibble() %&gt;% \n                                        dplyr::select(2) %&gt;% \n                                        quantile(probs = cutoff, na.rm = TRUE) %&gt;% \n                                        as.vector()\n                                      \n                                      final &lt;- cbind(PUs[,1], ofdates[,2], dist02) %&gt;% \n                                        as_tibble() %&gt;% \n                                        dplyr::select(-geometry, -geometry.1) %&gt;% \n                                        dplyr::arrange(.[[3]]) %&gt;% \n                                        dplyr::filter(.[[2]] &gt; qfront) %&gt;% \n                                        dplyr::slice(1)\n                                      \n                                      lsout[[x]] &lt;- final %&gt;% \n                                        dplyr::rename_with(.cols = 1, ~\"pxID\") %&gt;% \n                                        dplyr::rename_with(.cols = 2, ~\"FrontMag\") %&gt;% \n                                        dplyr::rename_with(.cols = 3, ~\"DistHFront\") %&gt;% \n                                        dplyr::mutate(group = as.character(unique(fauna$birdID)), \n                                                      dates = unique(fauna$date))\n                                      \n                                      \n                                      \n                                    }\n    stopCluster(cl)\n    FFF &lt;- do.call(rbind, ls_out)\n    return(FFF)\n  }\n\n\n\n\n\n\n\n\n\nworking with the distance function on megafauna and ocean front data\n\n\n\n\n# Define file paths and parameters\npus = \"input_layers/boundaries/PUs_NA_04km2.shp\" # The grid with empty cells\nfsle_sf = \"data_rout/dt_NA_allsat_madt_fsle_2021-07.rds\" # The 2021 July FSLE data\nfdata = \"inputs_sb/sb_LSP_F\" # The foraging data\ncutoff = 0.75 # The cutoff value to define what qualifies as an ocean front\noutput = \"outputs_sb/\" # The directory where the analysis results will be saved\n\n# Read the grid (PUs) shapefile\nPUs &lt;- st_read(pus)\n\n# Load and prepare the FSLE data\nsf1 &lt;- readRDS(fsle_sf) %&gt;% \n  dplyr::mutate(across(everything(), ~ .x * -1)) # Negate the values as required by the analysis\n\n# Extract and clean the names of the FSLE data columns\nnms &lt;- names(sf1) %&gt;% \n  stringr::str_extract(pattern = \".*(?=\\\\.)\") # Extract everything before the first period (.) in each column name\ncolnames(sf1) &lt;- nms # Assign cleaned names to the FSLE data columns\n\n# Get the list of foraging data files\nvecFls &lt;- list.files(path = fdata, pattern = \".rds\", all.files = TRUE, full.names = TRUE, recursive = FALSE)\n\n# Extract the date from the FSLE file name to match with the correct foraging data\nOFdates &lt;- stringr::str_remove(unlist(stringr::str_split(basename(fsle_sf), pattern = \"_\"))[6], pattern = \".rds\") \n\n# Filter the foraging data files to only include those that match the FSLE data date\nvecFls &lt;- vecFls[stringr::str_detect(string = vecFls, pattern = OFdates) == TRUE]\n\n# Define the coordinate reference systems (CRS) for the spatial data\nLatLon &lt;- \"EPSG:4326\" # Standard geographic coordinate system\nrobin &lt;- \"ESRI:54030\" # Robinson projection, often used for global maps\n\n# Read the first foraging data file\nfdata01 &lt;- readRDS(vecFls[1])\n\n# Ensure the data is a data frame (sometimes it might be a list)\nif(is.data.frame(fdata01)) {\n  fdata01\n} else {\n  fdata01 &lt;- fdata01[[1]] # If it's a list, extract the first element\n}\n\n# Extract the relevant FSLE data for the dates in the foraging dataset\ndf01 &lt;- sf1 %&gt;% \n  dplyr::select(as.character(unique(fdata01$date))) # Select columns that match the dates in the foraging data\n\n# Create a vector of unique dates from the foraging data\nFdates &lt;- unique(fdata01$date)\nFF &lt;- vector(\"list\", length = length(Fdates)) # Initialize a list to store the output for each date\n\n# Loop through each date to calculate the distance between megafauna records and ocean fronts\nfor(j in seq_along(Fdates)) {\n  \n  # Filter and transform the megafauna data for the current date\n  mmF &lt;- fdata01 %&gt;% \n    sf::st_as_sf(coords = c(\"x\", \"y\"), crs = LatLon) %&gt;% # Convert to spatial (sf) object\n    dplyr::filter(date == Fdates[1]) %&gt;% # Filter for the first date (as an example, we are doing this for the first one only; add `j` to iterate over all dates)\n    sf::st_transform(crs = robin) # Transform to Robinson projection\n  \n  # Filter and transform the ocean front data for the current date\n  OFCdates &lt;- df01 %&gt;% \n    dplyr::select(as.character(Fdates[1])) # Filter for the first date (as an example, we are doing this for the first one only; add `j` to iterate over all dates)\n  OFCdates &lt;- cbind(PUs, OFCdates) %&gt;% \n    st_transform(crs = robin) # Combine with the grid and transform to Robinson projection\n  \n  # Calculate the distance between megafauna records and ocean fronts\n  FF[[j]] &lt;- dist_fx(PUs = PUs, fauna = mmF, ofdates = OFCdates, cutoff = cutoff) # Store the result in the list\n}\n\n# Combine all the results into a single data frame\nFFdf &lt;- do.call(rbind, FF) # Combine the list of results into a single data frame\n\nThe output should look like this:\n\n\n\n\n\n\n\n\n\n\npxID: The identifier for the pixel in the grid\nFrontMag: The magnitude of the ocean front, in this case, the FSLE value\nDistHFront: The distance to the nearest ocean front\ngroup: The identifier for the bird, referred to as birdID\ndates: The date associated with the data, including the year, month, and day\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe NULL elements above correspond to the remaining dates we did not evaluate in the loop. By replacing the placeholder with j, you should get a complete list of elements in the list of data frames.\n\n\n\n\n\n\n\n\nf03_OFrontDist_SB.R function\n\n\n\n\n\nThe entire function above is located in the zscripts directory. You can also find it below if you want to paste it into a new script in your RStudio console.\n\n# This code was written by Isaac Brito-Morales (ibrito@conservation.org)\n# Please do not distribute this code without permission.\n# NO GUARANTEES THAT CODE IS CORRECT\n# Caveat Emptor!\n\nsource(\"zscripts/z_helpFX.R\")\n\nneardist_sim &lt;- function(pus, fsle_sf, fdata, cutoff, output) {\n  \n  library(sf)\n  library(terra)\n  library(stringr)\n  library(dplyr)\n  library(data.table)\n  library(future.apply)\n  library(parallel)\n  library(doParallel)\n  library(foreach)\n  \n  dist_fx &lt;- function(PUs, fauna, ofdates, cutoff) {\n    \n    UseCores &lt;- 5 # TO 5 IN LOCAL\n    cl &lt;- parallel::makeCluster(UseCores)\n    doParallel::registerDoParallel(cl)\n    lsout &lt;- vector(\"list\", length = nrow(fauna))\n    ls_out &lt;- foreach(x = 1:nrow(fauna),\n                      .packages = c(\"terra\",\n                                    \"dplyr\", \n                                    \"sf\", \n                                    \"stringr\")) %dopar% {\n                                      dist02 &lt;- st_distance(fauna[x, ], ofdates, by_element = FALSE) %&gt;%\n                                        t() %&gt;%\n                                        as_tibble()\n                                      # Get the upper front quantile of front\n                                      qfront &lt;- ofdates %&gt;%\n                                        as_tibble() %&gt;% \n                                        dplyr::select(2) %&gt;% \n                                        quantile(probs = cutoff, na.rm = TRUE) %&gt;% \n                                        as.vector()\n                                      \n                                      final &lt;- cbind(PUs[,1], ofdates[,2], dist02) %&gt;% \n                                        as_tibble() %&gt;% \n                                        dplyr::select(-geometry, -geometry.1) %&gt;% \n                                        dplyr::arrange(.[[3]]) %&gt;% \n                                        dplyr::filter(.[[2]] &gt; qfront) %&gt;% \n                                        dplyr::slice(1)\n                                      \n                                      lsout[[x]] &lt;- final %&gt;% \n                                        dplyr::rename_with(.cols = 1, ~\"pxID\") %&gt;% \n                                        dplyr::rename_with(.cols = 2, ~\"FrontMag\") %&gt;% \n                                        dplyr::rename_with(.cols = 3, ~\"DistHFront\") %&gt;% \n                                        dplyr::mutate(group = as.character(unique(fauna$birdID)), \n                                                      dates = unique(fauna$date))\n                                      \n                                      \n                                      \n                                    }\n    stopCluster(cl)\n    FFF &lt;- do.call(rbind, ls_out)\n    return(FFF)\n  }\n  \n  # Reading inputs\n    PUs &lt;- st_read(pus)\n    sf1 &lt;- readRDS(fsle_sf) %&gt;% \n      dplyr::mutate(across(everything(), ~ .x * -1))\n    nms &lt;- names(sf1) %&gt;% \n      stringr::str_extract(pattern = \".*(?=\\\\.)\")\n    colnames(sf1) &lt;- nms\n  # \n    vecFls &lt;- list.files(path = fdata, pattern = \".rds\", all.files = TRUE, full.names = TRUE, recursive = FALSE)\n    OFdates &lt;- stringr::str_remove(unlist(stringr::str_split(basename(fsle_sf), pattern = \"_\"))[6], pattern = \".rds\")\n    vecFls &lt;- vecFls[stringr::str_detect(string = vecFls, pattern = OFdates) == TRUE]\n    \n    FFdf &lt;-  future.apply::future_lapply(vecFls, future.scheduling = 5, FUN = function(x) { # TO 5 IN LOCAL\n      # \n        LatLon &lt;- \"EPSG:4326\"\n        robin &lt;- \"ESRI:54030\"\n      # \n        fdata01 &lt;- readRDS(x)\n        if(is.data.frame(fdata01)) {\n          fdata01\n          } else {\n            fdata01 &lt;- fdata01[[1]]\n          }\n      # ocean fronts per day of each biodiversity data\n      df01 &lt;- sf1 %&gt;% \n        dplyr::select(as.character(unique(fdata01$date)))\n      # we need to create a vector with the unique date information\n      Fdates &lt;- unique(fdata01$date)\n      FF &lt;- vector(\"list\", length = length(Fdates))\n      for(j in seq_along(Fdates)) {\n        # Filter the megafauna data for each date\n        mmF &lt;- fdata01 %&gt;% \n          sf::st_as_sf(coords = c(\"x\", \"y\"), crs = LatLon) %&gt;% # from dataframe to sf object\n          dplyr::filter(date == Fdates[j]) %&gt;% # add the j here\n          sf::st_transform(crs = robin)\n        # Filter Front data for each date of the megafauna data\n        OFCdates &lt;- df01 %&gt;% \n          dplyr::select(as.character(Fdates[j]))\n        OFCdates &lt;- cbind(PUs, OFCdates) %&gt;% \n          st_transform(crs = robin)\n        # \n        FF[[j]] &lt;- dist_fx(PUs = PUs, fauna = mmF, ofdates = OFCdates, cutoff = cutoff)\n      }\n      # Tidy up the final list\n      FFdf &lt;- do.call(rbind, FF)\n      })\n    \n    \n  # File name for the output\n    lapply(FFdf, function(x){\n      sgl &lt;- x\n      ngrd &lt;- paste0(unlist(stringr::str_split(basename(output), \"_\"))[2:3], collapse = \"_\")\n      ndate &lt;- paste0(unlist(stringr::str_split(unlist(unique(sgl$dates)[1]), \"-\"))[1:2], collapse = \"-\")\n      ffname &lt;- paste(ngrd, ndate, unique(gsub(\"[./]\", \"\", sgl$group)), sep = \"_\")\n      saveRDS(sgl, paste0(output, ffname, paste(\"_cutoff\", cutoff, sep = \"-\"), \".rds\"))\n    })\n    return(FFdf)\n}\n\n# Focus on -&gt; 2021-07; 2021-08\n# SeaBirds\n   system.time(tt &lt;- neardist_sim(pus = \"input_layers/boundaries/PUs_NA_04km2.shp\",\n                                  fsle_sf = \"data_rout/dt_NA_allsat_madt_fsle_2021-08.rds\",\n                                  fdata = \"inputs_sb/sb_LSP_T\",\n                                  cutoff = 0.75,\n                                  output = \"outputs_sb/sb_LSP_T/\"))"
  },
  {
    "objectID": "10_eval_hist_plots.html#merging-the-output-data",
    "href": "10_eval_hist_plots.html#merging-the-output-data",
    "title": "10  Distance Plots",
    "section": "10.1 Merging the output data",
    "text": "10.1 Merging the output data\nThe function (and script) below will create a combined list() containing two elements.\n\nThe first element will be a combined dataframe of the ocean front analyses by birdID\nThe second element will be a dataframe that provides distance information, split by Foraging and Transit behaviors\n\nThis approach allows for greater control and makes the process more efficient when generating plots and other outputs.\n\n\n\n\n\n\nForagingTransit function\n\n\n\nThe function requires only two arguments:\n\nindirFor: directory where the ocean front analyses for the foraging behavior are stored (e.g., outputs_sb/sb_LSP_F).\nindirTran: directory where the ocean front analyses for the transit behavior are stored (e.g., outputs_sb/sb_LSP_T).\n\n\n# Source the helper functions and input files\n\nsource(\"zscripts/z_helpFX.R\")  # Load helper functions from the script\nsource(\"zscripts/z_inputFls_local.R\")  # Load input files specific to this workshop\n\nForagingTransit &lt;- function(indirFor, indirTran) {\n  \n  # Load required libraries\n  library(dplyr)       # For data manipulation\n  library(ggplot2)     # For data visualization\n  library(units)       # For handling units of measurement\n  library(ggtext)      # For enhanced text rendering in ggplot2\n  library(gdata)       # For combining data frames of different lengths\n  \n  # List all .rds files in the foraging behavior directory\n  lsO &lt;- list.files(path = indirFor, \n                    pattern = \".rds\", \n                    all.files = TRUE, \n                    full.names = TRUE, \n                    recursive = FALSE)\n  \n  # List all .rds files in the transit behavior directory\n  lsS &lt;- list.files(path = indirTran, \n                    pattern = \".rds\", \n                    all.files = TRUE, \n                    full.names = TRUE, \n                    recursive = FALSE)\n  \n  # Data manipulation for the original (foraging) data\n  dfOr &lt;- lapply(lsO, function(x) {\n    sgl &lt;- readRDS(x)  # Read each .rds file\n    sgl$DistHFront &lt;- units::set_units(sgl$DistHFront, \"km\")  # Set the distance to ocean front in kilometers\n    sgl})\n  dfOr &lt;- do.call(rbind, dfOr) %&gt;%  # Combine all foraging data into a single dataframe\n    as_tibble() %&gt;% \n    dplyr::mutate(data = \"original\")  # Add a column to label this as original data\n  \n  # Data manipulation for the simulation (transit) data\n  dfS &lt;- lapply(lsS, function(x) {\n    sgl &lt;- readRDS(x)  # Read each .rds file\n    sgl$DistHFront &lt;- units::set_units(sgl$DistHFront, \"km\")  # Set the distance to ocean front in kilometers\n    sgl})\n  dfS &lt;- do.call(rbind, dfS) %&gt;%  # Combine all transit data into a single dataframe\n    as_tibble() %&gt;% \n    dplyr::mutate(data = \"simulation\")  # Add a column to label this as simulation data\n  \n  # Merge both datasets (original and simulation)\n  DFF &lt;- rbind(dfOr, dfS) %&gt;%  # Combine original and simulation data into one dataframe\n    as_tibble()\n  \n  # Prepare for comparison between Foraging and Transit distances\n  FO &lt;- dfOr %&gt;% \n    dplyr::rename(DistHFrontF = DistHFront) %&gt;%  # Rename the distance column for foraging data\n    dplyr::select(DistHFrontF)\n  \n  FS &lt;- dfS %&gt;% \n    dplyr::mutate(DistHFrontT = DistHFront) %&gt;%  # Rename the distance column for transit data\n    dplyr::select(DistHFrontT)\n  \n  # Handle cases where the number of rows in Foraging and Transit datasets differ\n  if(nrow(FO) != nrow(FS)) {\n    FF &lt;- gdata::cbindX(FO, FS) %&gt;%  # Combine dataframes with different lengths, allowing for missing values\n      as_tibble() %&gt;% \n      dplyr::mutate(DistHFrontF = as.numeric(DistHFrontF),  # Convert the distances to numeric\n                    DistHFrontT = as.numeric(DistHFrontT))\n    dfF.ls &lt;- list(DFF, FF)  # Store both the merged data and the comparison in a list\n  } else {\n    FF &lt;- cbind(FO, FS) %&gt;%  # Combine the dataframes directly if they have the same number of rows\n      as_tibble() %&gt;% \n      dplyr::mutate(DistHFrontF = as.numeric(DistHFrontF),  # Convert the distances to numeric\n                    DistHFrontT = as.numeric(DistHFrontT))\n    dfF.ls &lt;- list(DFF, FF)  # Store both the merged data and the comparison in a list\n  }\n  \n  # Return the list containing both the full merged dataframe and the comparison dataframe\n  return(dfF.ls)\n}\n\nTry running the following code:\n\ndfF &lt;- ForagingTransit(indirFor = \"outputs_sb/sb_LSP_F\", \n                       indirTran = \"outputs_sb/sb_LSP_T\")\n\nThe final output should look like this:"
  },
  {
    "objectID": "10_eval_hist_plots.html#kernel-density-plot",
    "href": "10_eval_hist_plots.html#kernel-density-plot",
    "title": "10  Distance Plots",
    "section": "10.2 kernel density plot",
    "text": "10.2 kernel density plot\nTo create a kernel density plot we will use ggplot R package\n\n\n\n\n\n\nread the kernel_ggplot function in zscripts/f04_MergeClean.R\n\n\n\n\n\nThe entire function above is located in the zscripts directory. You can also find it below if you want to paste it into a new script in your RStudio console. This function will require three argument:\n\ninput: the output from the ForagingTransit function\nxlabs: labels on the x axis\nylabs: labels on the y axis\n\n1. Define General Settings for the Plot Output\n\ntheme_op01 &lt;- theme(plot.title = element_text(face = \"plain\", size = 20, hjust = 0.5),\n                    plot.tag = element_text(colour = \"black\", face = \"bold\", size = 23),\n                    axis.title.x = element_text(size = rel(1.5), angle = 0),\n                    axis.text.x = element_text(size = rel(2), angle = 0),\n                    axis.title.y = element_text(size = rel(1.5), angle = 90),\n                    axis.text.y = element_text(size = rel(2), angle = 0),\n                    legend.title = element_text(colour = \"black\", face = \"bold\", size = 15),\n                    legend.text = element_text(colour = \"black\", face = \"bold\", size = 13),\n                    legend.key.height = unit(1.5, \"cm\"),\n                    legend.key.width = unit(1.5, \"cm\"))\n\n2. kernel_ggplot ggplot function\n\nkernel_ggplot &lt;- function(input, xlabs, ylabs) {\n  \n  # Extract the first and second elements from the input list\n  df01 &lt;- input[[1]]  # This is the combined dataframe of ocean front analyses by birdID\n  df02 &lt;- input[[2]]  # This is the dataframe containing distance information, split by Foraging and Transit\n  \n  # Create a ggplot object\n  ggp01 &lt;- ggplot(df02, aes(x = x)) +\n    \n    # Plot the density curve for the Foraging data (positive density on the top side)\n    geom_density(aes(x = DistHFrontO, y = after_stat(density)), \n                 lwd = 1,  # Line width\n                 colour = \"#1f77b4\",  # Line color\n                 fill = \"#1f77b4\",  # Fill color\n                 alpha = 0.50,  # Transparency level\n                 adjust = 0.5) +  # Smoothing parameter\n    \n    # Plot the density curve for the Transit data (negative density on the bottom side)\n    geom_density(aes(x = DistHFrontS, y = after_stat(-density)), \n                 lwd = 1,  # Line width\n                 colour = \"#a8ddb5\",  # Line color\n                 fill = \"#a8ddb5\",  # Fill color\n                 alpha = 0.50,  # Transparency level\n                 adjust = 0.5) +  # Smoothing parameter\n    \n    # Add a horizontal dashed line at y = 0 to separate the top and bottom density plots\n    geom_hline(yintercept = 0, colour = \"black\", linetype = \"dashed\") +\n    \n    # Adjust the x-axis scale to remove extra padding\n    scale_x_continuous(expand = c(0, 0)) +\n    \n    # Adjust the y-axis to display absolute values, showing the density on both sides\n    scale_y_continuous(breaks = seq(-0.4, 0.4, 0.1), \n                       limits = c(-0.32, 0.32), \n                       expand = c(0, 0), \n                       labels = function(x) abs(x)) +\n    \n    # Set the x-axis limits to zoom in on the relevant data range\n    coord_cartesian(xlim = c(0, 20)) +\n    \n    # Add labels for the x-axis and y-axis using the provided arguments\n    labs(x = xlabs,\n         y = ylabs) +\n    \n    # Apply a specific theme for consistent styling\n    theme_op01 +  # Apply a custom theme (assuming 'theme_op01' is defined elsewhere)\n    theme_bw() +  # Apply a basic white background theme\n    \n    # Add a label to the top plot (Foraging data) showing the sample size and type of data\n    geom_richtext(inherit.aes = FALSE, \n                  data = tibble(x = 5, y = 0.2, \n                                label = paste(\"n =\", length(unique(df01$group)), \"&lt;br&gt;type = Foraging\")),\n                  aes(x = x, y = y, label = label), \n                  size = 3.5,\n                  fill = \"white\", \n                  colour = \"#1f77b4\",  # Text color matching the Foraging plot\n                  label.color = \"black\",\n                  hjust = 0) +\n    \n    # Add a label to the bottom plot (Transit data) showing the sample size and type of data\n    geom_richtext(inherit.aes = FALSE,\n                  data = tibble(x = 5, y = -0.2,\n                                label = paste(\"n =\", length(unique(df01$group)), \"&lt;br&gt;data = Transit\")),\n                  aes(x = x, y = y, label = label),\n                  size = 3.5,\n                  fill = \"white\",\n                  colour = \"#a8ddb5\",  # Text color matching the Transit plot\n                  label.color = \"black\",\n                  hjust = 0)\n  \n  # Return the completed ggplot object\n  return(ggp01)\n}\n\n\n\n\nThe kernel_ggplot function, located inzscripts/f04_MergeClean.R, is straightforward to understand and use. To generate the plot, simply run the following command:\n\n# Generate a kernel density plot using the custom kernel_ggplot function\ngg_dfF &lt;- kernel_ggplot(input = dfF, \n                        xlabs = \"Distance to high FSLE (km)\",  # Label for the x-axis\n                        ylabs = \"Density\")  # Label for the y-axis\n\n# Save the generated plot as a PNG file\nggsave(\"figures/LSP_2021-07.png\",  # File path and name for the output image\n       plot = gg_dfF,  # The plot object to save\n       width = 10,  # Width of the output image in inches\n       height = 6,  # Height of the output image in inches\n       dpi = 300,  # Resolution of the image in dots per inch (high quality)\n       limitsize = FALSE)  # Allow saving of images larger than the default size limits"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "11  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Schoeman, David S., Alex Sen Gupta, Cheryl S. Harrison, Jason D.\nEverett, Isaac Brito-Morales, Lee Hannah, Laurent Bopp, Patrick R.\nRoehrdanz, and Anthony J. Richardson. 2023. “Demystifying Global\nClimate Models for Use in the Life Sciences.” Trends in\nEcology & Evolution 38 (9): 843–58. https://doi.org/10.1016/j.tree.2023.04.005.\n\n\nSudre, Floriane, Boris Dewitte, Camille Mazoyer, Véronique Garçon, Joel\nSudre, Pierrick Penven, and Vincent Rossi. 2023. “Spatial and\nSeasonal Variability of Horizontal Temperature Fronts in the Mozambique\nChannel for Both Epipelagic and Mesopelagic Realms.”\nFrontiers in Marine Science 9. https://www.frontiersin.org/articles/10.3389/fmars.2022.1045136.\n\n\nSudre, Floriane, Ismael Hernández-Carrasco, Camille Mazoyer, Joel Sudre,\nBoris Dewitte, Véronique Garçon, and Vincent Rossi. 2023a. “An\nOcean Front Dataset for the Mediterranean Sea and Southwest Indian\nOcean.” Scientific Data 10 (1): 730. https://doi.org/10.1038/s41597-023-02615-z.\n\n\n———. 2023b. “An Ocean Front Dataset for the Mediterranean Sea and\nSouthwest Indian Ocean.” Scientific Data 10 (1): 730. https://doi.org/10.1038/s41597-023-02615-z."
  }
]